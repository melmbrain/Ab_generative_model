"""
Validate Generated Antibodies

This script validates antibodies generated by your model using ESM-2.
It computes perplexity scores to measure how "natural" the sequences are
according to a pre-trained protein language model.

This is a well-established validation method used in papers like IgLM (2023).

Usage:
    # Validate antibodies from a trained model
    python validate_antibodies.py --checkpoint checkpoints/improved_small_2025_10_31_best.pt

    # Validate with custom settings
    python validate_antibodies.py --checkpoint checkpoints/best.pt --num-samples 50 --device cpu

Lower perplexity = more natural sequence (better quality)
"""

import argparse
import torch
import json
from pathlib import Path
import sys

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent))

from generators.transformer_seq2seq import create_model
from generators.tokenizer import AminoAcidTokenizer
from generators.data_loader import AbAgDataset


def load_model_and_generate(checkpoint_path, num_samples=20, device='cuda'):
    """
    Load trained model and generate antibody sequences

    Args:
        checkpoint_path: Path to model checkpoint
        num_samples: Number of antibodies to generate
        device: 'cuda' or 'cpu'

    Returns:
        List of (antibody_sequence, antigen_sequence, target_pkd) tuples
    """
    print("\n" + "="*70)
    print("Loading Model and Generating Antibodies")
    print("="*70)

    # Load tokenizer
    print("\n1. Loading tokenizer...")
    tokenizer = AminoAcidTokenizer()
    print(f"   Vocab size: {tokenizer.vocab_size}")

    # Load validation data
    print("\n2. Loading validation data...")
    val_dataset = AbAgDataset(
        data_path='data/generative/val.json',
        tokenizer=tokenizer
    )
    print(f"   Validation samples: {len(val_dataset)}")

    # Create model
    print("\n3. Creating model...")
    model = create_model('small', vocab_size=tokenizer.vocab_size, max_src_len=512, max_tgt_len=300)
    print(f"   Parameters: {model.get_model_size():,}")

    # Load checkpoint
    print(f"\n4. Loading checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()

    print(f"   Loaded from epoch {checkpoint['epoch']}")
    print(f"   Validation loss: {checkpoint['val_loss']:.4f}")

    # Generate antibodies
    print(f"\n5. Generating {num_samples} antibodies...")
    generated_antibodies = []

    with torch.no_grad():
        for i in range(min(num_samples, len(val_dataset))):
            # Get sample
            sample = val_dataset[i]

            # Tokenize antigen sequence
            antigen_seq = sample['antigen_sequence']
            antigen_tokens = tokenizer.encode(antigen_seq)
            # Truncate to max length if needed
            if len(antigen_tokens) > 512:
                antigen_tokens = antigen_tokens[:512]
            antigen_tokens_tensor = torch.tensor([antigen_tokens]).to(device)

            # Prepare pKd
            pkd = torch.tensor([[sample['pKd']]]).float().to(device)

            # Generate
            generated = model.generate_greedy(antigen_tokens_tensor, pkd, max_length=300)

            # Decode
            antibody_seq = tokenizer.decode(generated[0].cpu().tolist())

            generated_antibodies.append((
                antibody_seq,
                antigen_seq,
                float(sample['pKd'])
            ))

            if (i + 1) % 10 == 0:
                print(f"   Generated {i + 1}/{num_samples}...")

    print(f"\n✅ Generated {len(generated_antibodies)} antibodies")

    return generated_antibodies


def validate_with_esm2(antibodies, device='cuda'):
    """
    Validate antibodies using ESM-2 (perplexity-based validation)

    This method uses ESM-2 to compute perplexity scores, which measure how
    "natural" the generated sequences are according to a pre-trained protein
    language model. Lower perplexity = more natural sequence.

    This is a well-established validation method used in papers like IgLM.

    Args:
        antibodies: List of (antibody, antigen, pkd) tuples
        device: 'cuda' or 'cpu'

    Returns:
        List of validation results
    """
    print("\n" + "="*70)
    print("Validating Antibodies with ESM-2")
    print("="*70)

    try:
        # Import ESM
        import esm

        print("\nLoading ESM-2 model (650M parameters)...")
        model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()
        batch_converter = alphabet.get_batch_converter()
        model = model.eval()

        if device == 'cuda' and torch.cuda.is_available():
            model = model.cuda()
            print("✅ ESM-2 loaded on GPU")
        else:
            print("✅ ESM-2 loaded on CPU (slower)")

    except ImportError:
        print("\n❌ ESM not installed!")
        print("\nTo install:")
        print("  pip install fair-esm")
        return []
    except Exception as e:
        print(f"\n❌ Error loading ESM-2: {e}")
        return []

    # Validate each antibody
    results = []

    print(f"\nValidating {len(antibodies)} antibodies...")
    print("(Computing perplexity scores...)\n")

    for i, (antibody, antigen, pkd) in enumerate(antibodies):
        print(f"[{i+1}/{len(antibodies)}] Validating antibody {i+1}...")

        try:
            # Remove separator
            seq_clean = antibody.replace('|', '')

            # Prepare data for ESM-2
            data = [("antibody", seq_clean)]
            batch_labels, batch_strs, batch_tokens = batch_converter(data)

            if device == 'cuda' and torch.cuda.is_available():
                batch_tokens = batch_tokens.cuda()

            # Compute perplexity
            with torch.no_grad():
                results_dict = model(batch_tokens, repr_layers=[33], return_contacts=False)
                logits = results_dict['logits']

                # Calculate perplexity (excluding special tokens)
                # Lower perplexity = more natural sequence
                log_probs = torch.nn.functional.log_softmax(logits, dim=-1)

                # Get log probability of actual tokens
                actual_tokens = batch_tokens[:, 1:-1]  # Exclude BOS/EOS
                log_probs = log_probs[:, :-2, :]  # Align dimensions

                token_log_probs = log_probs.gather(2, actual_tokens.unsqueeze(-1)).squeeze(-1)
                mean_log_prob = token_log_probs.mean().item()
                perplexity = float(torch.exp(-torch.tensor(mean_log_prob)))

            # Determine quality grade based on perplexity
            # Lower is better for perplexity
            if perplexity < 5:
                quality = "Excellent"
                is_good = True
            elif perplexity < 10:
                quality = "Good"
                is_good = True
            elif perplexity < 20:
                quality = "Fair"
                is_good = False
            else:
                quality = "Poor"
                is_good = False

            result = {
                'antibody_id': i,
                'sequence': antibody,
                'antigen': antigen[:50] + '...',  # Truncate for readability
                'target_pkd': pkd,
                'length': len(seq_clean),
                'perplexity': perplexity,
                'mean_log_prob': mean_log_prob,
                'is_good_sequence': is_good,
                'quality_grade': quality
            }

            results.append(result)

            # Print result
            status = "✅" if result['is_good_sequence'] else "⚠️ "
            print(f"    Perplexity: {perplexity:.2f} - {quality} {status}")

        except Exception as e:
            print(f"    ❌ Error: {e}")
            results.append({
                'antibody_id': i,
                'sequence': antibody,
                'error': str(e)
            })

    return results


def extract_plddt_from_pdb(pdb_string):
    """Extract pLDDT scores from PDB string"""
    import numpy as np

    residue_plddt = {}

    for line in pdb_string.split('\n'):
        if line.startswith('ATOM'):
            try:
                res_num = int(line[22:26].strip())
                plddt = float(line[60:66].strip())
                if res_num not in residue_plddt:
                    residue_plddt[res_num] = plddt
            except:
                pass

    return np.array(list(residue_plddt.values()))


def get_quality_grade(plddt):
    """Get quality grade based on pLDDT score"""
    if plddt > 90:
        return "Excellent"
    elif plddt > 70:
        return "Good"
    elif plddt > 50:
        return "Fair"
    else:
        return "Poor"


def save_results(results, output_dir='validation_results'):
    """Save validation results"""
    import numpy as np

    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)

    # Save detailed results
    results_file = output_path / 'validation_results.json'
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"\n✅ Detailed results saved to: {results_file}")

    # Calculate summary statistics
    valid_results = [r for r in results if 'error' not in r]

    if valid_results:
        perplexity_scores = [r['perplexity'] for r in valid_results]

        summary = {
            'total_antibodies': len(results),
            'successful_predictions': len(valid_results),
            'failed_predictions': len(results) - len(valid_results),
            'mean_perplexity': float(np.mean(perplexity_scores)),
            'std_perplexity': float(np.std(perplexity_scores)),
            'median_perplexity': float(np.median(perplexity_scores)),
            'min_perplexity': float(np.min(perplexity_scores)),
            'max_perplexity': float(np.max(perplexity_scores)),
            'excellent_sequences': sum(1 for p in perplexity_scores if p < 5),
            'good_sequences': sum(1 for p in perplexity_scores if 5 <= p < 10),
            'fair_sequences': sum(1 for p in perplexity_scores if 10 <= p < 20),
            'poor_sequences': sum(1 for p in perplexity_scores if p >= 20),
            'quality_distribution': {
                'Excellent (<5)': sum(1 for p in perplexity_scores if p < 5),
                'Good (5-10)': sum(1 for p in perplexity_scores if 5 <= p < 10),
                'Fair (10-20)': sum(1 for p in perplexity_scores if 10 <= p < 20),
                'Poor (>=20)': sum(1 for p in perplexity_scores if p >= 20)
            }
        }

        # Save summary
        summary_file = output_path / 'validation_summary.json'
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)

        # Print summary
        print("\n" + "="*70)
        print("Validation Summary (ESM-2 Perplexity)")
        print("="*70)
        print(f"Total antibodies:         {summary['total_antibodies']}")
        print(f"Successful predictions:   {summary['successful_predictions']}")
        print(f"Failed predictions:       {summary['failed_predictions']}")
        print(f"\nSequence Quality (Lower perplexity = Better):")
        print(f"  Mean Perplexity:        {summary['mean_perplexity']:.2f} ± {summary['std_perplexity']:.2f}")
        print(f"  Median Perplexity:      {summary['median_perplexity']:.2f}")
        print(f"  Range:                  {summary['min_perplexity']:.2f} - {summary['max_perplexity']:.2f}")
        print(f"\nQuality Distribution:")
        print(f"  Excellent (<5):         {summary['excellent_sequences']} ({summary['excellent_sequences']/len(valid_results)*100:.1f}%)")
        print(f"  Good (5-10):            {summary['good_sequences']} ({summary['good_sequences']/len(valid_results)*100:.1f}%)")
        print(f"  Fair (10-20):           {summary['fair_sequences']} ({summary['fair_sequences']/len(valid_results)*100:.1f}%)")
        print(f"  Poor (>=20):            {summary['poor_sequences']} ({summary['poor_sequences']/len(valid_results)*100:.1f}%)")
        print("="*70)

        print(f"\n✅ Summary saved to: {summary_file}")

    else:
        print("\n⚠️  No valid results to summarize")


def main():
    parser = argparse.ArgumentParser(description='Validate generated antibodies')
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='Path to model checkpoint')
    parser.add_argument('--num-samples', type=int, default=20,
                       help='Number of antibodies to validate (default: 20)')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device to use: cuda or cpu (default: cuda)')
    parser.add_argument('--output-dir', type=str, default='validation_results',
                       help='Output directory for results (default: validation_results)')

    args = parser.parse_args()

    print("\n" + "="*70)
    print("Antibody Validation Pipeline")
    print("="*70)
    print(f"Checkpoint: {args.checkpoint}")
    print(f"Samples: {args.num_samples}")
    print(f"Device: {args.device}")
    print(f"Output: {args.output_dir}")

    # Step 1: Generate antibodies
    antibodies = load_model_and_generate(
        args.checkpoint,
        num_samples=args.num_samples,
        device=args.device
    )

    if not antibodies:
        print("\n❌ No antibodies generated. Exiting.")
        return

    # Step 2: Validate with ESM-2
    results = validate_with_esm2(antibodies, device=args.device)

    if not results:
        print("\n❌ Validation failed. Exiting.")
        return

    # Step 3: Save results
    save_results(results, output_dir=args.output_dir)

    print("\n✅ Validation complete!")
    print(f"\nResults saved in: {args.output_dir}/")


if __name__ == '__main__':
    main()

# Research Log & Complete References

**Project**: Affinity-Conditioned Antibody Generation Model
**Author**: [Your Name]
**Start Date**: October 31, 2024
**Last Updated**: November 3, 2024

---

## üìã Table of Contents

1. [Core Antibody Generation Models](#core-antibody-generation-models)
2. [Transformer Architecture & Deep Learning](#transformer-architecture)
3. [Optimization & Training Techniques](#optimization-techniques)
4. [Protein Structure Prediction](#protein-structure-prediction)
5. [Validation Methods & Benchmarking](#validation-methods)
6. [Datasets & Databases](#datasets-databases)
7. [Software & Tools](#software-tools)
8. [Implementation Timeline](#implementation-timeline)

---

## üß¨ Core Antibody Generation Models

### 1. PALM-H3 (2024) - PRIMARY REFERENCE

**Full Citation**:
> Liu, H., Dieckhaus, H., Hao, Y., Franceschi, F., Berner, J., Krawczyk, K., ... & others (2024). De novo generation of SARS-CoV-2 antibody CDRH3 with a pre-trained generative large language model. *Nature Communications*, 15(1), 7570.

**DOI**: [10.1038/s41467-024-50903-y](https://doi.org/10.1038/s41467-024-50903-y)

**What We Used**:
- Pre-Layer Normalization architecture
- GELU activation functions
- ESM2-inspired encoder architecture
- Validation methodology (AlphaFold, ESMFold)

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- Informed architecture choices (Pre-LN, GELU)
- Validated ESMFold as acceptable validation tool
- Showed antigen-conditioned generation is viable

---

### 2. IgLM (2023) - PRIMARY REFERENCE

**Full Citation**:
> Shuai, R. W., Ruffolo, J. A., & Gray, J. J. (2023). IgLM: Infilling language modeling for antibody sequence design. *Cell Systems*, 14(11), 979-989.

**DOI**: [10.1016/j.cels.2023.10.001](https://doi.org/10.1016/j.cels.2023.10.001)

**What We Used**:
- Validation metrics (validity, diversity)
- AlphaFold-Multimer validation approach
- Sequence evaluation methodology

**Training Data**: 558 million antibody sequences

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê
- Established baseline metrics for comparison
- Showed importance of diversity metrics
- Validated use of structure prediction for evaluation

---

### 3. IgT5 & IgBert (2024)

**Full Citation**:
> Burbach, S. M., & Briney, B. (2024). Paired sequence modeling of antibody repertoires using continuous distributed representations. *PLOS Computational Biology*, 20(12), e1012646.

**DOI**: [10.1371/journal.pcbi.1012646](https://doi.org/10.1371/journal.pcbi.1012646)

**What We Used**:
- Transformer architecture insights
- Paired heavy-light chain concepts
- Modern training techniques

**Training Data**: 2+ billion antibody sequences from OAS

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê
- Informed paired sequence handling
- Showed scale of pre-training possible
- Validated T5/Transformer approach

---

## üîß Transformer Architecture & Deep Learning

### 4. Attention Is All You Need (2017) - FOUNDATIONAL

**Full Citation**:
> Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. *Advances in Neural Information Processing Systems*, 30.

**arXiv**: [1706.03762](https://arxiv.org/abs/1706.03762)

**What We Used**:
- ‚úÖ Multi-head self-attention mechanism
- ‚úÖ Positional encoding (sinusoidal)
- ‚úÖ Encoder-decoder architecture
- ‚úÖ Scaled dot-product attention

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- **Core architecture basis**
- All transformer components derived from this paper
- Foundational to entire model design

---

### 5. RoFormer - Rotary Position Embeddings (2021)

**Full Citation**:
> Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., & Liu, Y. (2021). RoFormer: Enhanced transformer with rotary position embedding. *arXiv preprint arXiv:2104.09864*.

**arXiv**: [2104.09864](https://arxiv.org/abs/2104.09864)

**What We Considered**:
- Rotary Position Embeddings (RoPE)
- Better relative position modeling
- Improved length extrapolation

**Status**: ‚è≥ Identified for Phase 2 improvements

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê (Future work)

---

### 6. Pre-Layer Normalization (2020) - IMPLEMENTED

**Full Citation**:
> Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., ... & Liu, T. Y. (2020). On layer normalization in the transformer architecture. *International Conference on Machine Learning*, 10524-10533.

**arXiv**: [2002.04745](https://arxiv.org/abs/2002.04745)

**What We Used**:
- ‚úÖ **Pre-LN instead of Post-LN**
- ‚úÖ `norm_first=True` in PyTorch Transformer

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- **Critical for training stability**
- Faster convergence observed
- Standard in GPT-3, modern LLMs

---

### 7. BERT (2019) - IMPLEMENTED

**Full Citation**:
> Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. *Proceedings of NAACL-HLT*, 4171-4186.

**arXiv**: [1810.04805](https://arxiv.org/abs/1810.04805)

**What We Used**:
- ‚úÖ **GELU activation function**
- Pre-training paradigm insights

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê
- GELU used in all feedforward networks
- Better than ReLU for sequence modeling

---

### 8. GPT-2/3 (2019)

**Full Citation**:
> Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. *OpenAI Blog*, 1(8), 9.

**URL**: [OpenAI Research](https://openai.com/research/better-language-models)

**What We Used**:
- Pre-LN architecture validation
- Autoregressive generation concepts
- Training strategies

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê
- Confirmed Pre-LN + GELU combination
- Informed generation approach

---

## ‚öôÔ∏è Optimization & Training Techniques

### 9. GELU Activation (2016) - IMPLEMENTED

**Full Citation**:
> Hendrycks, D., & Gimpel, K. (2016). Gaussian error linear units (GELUs). *arXiv preprint arXiv:1606.08415*.

**arXiv**: [1606.08415](https://arxiv.org/abs/1606.08415)

**Formula**: `GELU(x) = x * Œ¶(x)`

**What We Used**:
- ‚úÖ **GELU in all transformer feedforward layers**
- ‚úÖ GELU in affinity projection network

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê
- Smoother gradients than ReLU
- Better for protein sequence modeling

---

### 10. Label Smoothing (2016) - IMPLEMENTED

**Full Citation**:
> Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., & Wojna, Z. (2016). Rethinking the inception architecture for computer vision. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, 2818-2826.

**arXiv**: [1512.00567](https://arxiv.org/abs/1512.00567)

**What We Used**:
- ‚úÖ **Label smoothing = 0.1 in CrossEntropyLoss**

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê
- Reduces overconfidence
- Better generalization
- Lower overfitting

---

### 11. Cosine LR Schedule with Warm Restarts (2017) - IMPLEMENTED

**Full Citation**:
> Loshchilov, I., & Hutter, F. (2017). SGDR: Stochastic gradient descent with warm restarts. *International Conference on Learning Representations*.

**arXiv**: [1608.03983](https://arxiv.org/abs/1608.03983)

**What We Used**:
- ‚úÖ **Linear warmup (10% of training)**
- ‚úÖ **Cosine decay to 10% of peak LR**

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- Prevents early training instability
- Smooth convergence
- Standard in modern LLM training

---

### 12. Adam Optimizer (2015)

**Full Citation**:
> Kingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimization. *International Conference on Learning Representations*.

**arXiv**: [1412.6980](https://arxiv.org/abs/1412.6980)

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê
- Foundation for AdamW

---

### 13. AdamW (2019) - IMPLEMENTED

**Full Citation**:
> Loshchilov, I., & Hutter, F. (2019). Decoupled weight decay regularization. *International Conference on Learning Representations*.

**arXiv**: [1711.05101](https://arxiv.org/abs/1711.05101)

**What We Used**:
- ‚úÖ **AdamW optimizer with weight_decay=0.01**

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê
- Better than Adam + L2 regularization
- Improved generalization

---

### 14. Gradient Clipping (2013) - IMPLEMENTED

**Full Citation**:
> Pascanu, R., Mikolov, T., & Bengio, Y. (2013). On the difficulty of training recurrent neural networks. *International Conference on Machine Learning*, 1310-1318.

**arXiv**: [1211.5063](https://arxiv.org/abs/1211.5063)

**What We Used**:
- ‚úÖ **Gradient clipping with max_norm=1.0**

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê
- Prevents gradient explosion
- Training stability

---

## üî¨ Protein Structure Prediction

### 15. AlphaFold2 (2021)

**Full Citation**:
> Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., ... & Hassabis, D. (2021). Highly accurate protein structure prediction with AlphaFold. *Nature*, 596(7873), 583-589.

**DOI**: [10.1038/s41586-021-03819-2](https://doi.org/10.1038/s41586-021-03819-2)

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê
- Informed structure validation approach
- Showed importance of structure quality

---

### 16. AlphaFold3 (2024) - REFERENCE

**Full Citation**:
> Abramson, J., Adler, J., Dunger, J., Evans, R., Green, T., Pritzel, A., ... & others (2024). Accurate structure prediction of biomolecular interactions with AlphaFold 3. *Nature*, 630(8016), 493-500.

**DOI**: [10.1038/s41586-024-07487-w](https://doi.org/10.1038/s41586-024-07487-w)

**Status**: ‚è≥ Not publicly available for automated use

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê (Future validation)
- Will enable antibody-antigen binding prediction
- Currently only web server available

---

### 17. ESM-2 & ESMFold (2022) - IMPLEMENTED

**Full Citation**:
> Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., ... & others (2022). Language models of protein sequences at the scale of evolution enable accurate structure prediction. *bioRxiv*.

**DOI**: [10.1101/2022.07.20.500902](https://doi.org/10.1101/2022.07.20.500902)

**What We Used**:
- ‚úÖ **ESMFold for structure validation**
- ‚úÖ pLDDT confidence scores

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- **PRIMARY validation tool**
- Fast (1 second per structure)
- Accurate enough for antibody validation
- Publicly available and easy to use

---

### 18. IgFold (2023) - REFERENCE

**Full Citation**:
> Ruffolo, J. A., Chu, L. S., Mahajan, S. P., & Gray, J. J. (2023). Fast, accurate antibody structure prediction from deep learning on massive set of natural antibodies. *Nature Communications*, 14(1), 2389.

**DOI**: [10.1038/s41467-023-38063-x](https://doi.org/10.1038/s41467-023-38063-x)

**Status**: ‚è≥ Identified as potential enhancement

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê (Future work)
- Antibody-specific validation
- Could complement ESMFold

---

### 19. ProteinMPNN (2022)

**Full Citation**:
> Dauparas, J., Anishchenko, I., Bennett, N., Bai, H., Ragotte, R. J., Milles, L. F., ... & others (2022). Robust deep learning-based protein sequence design using ProteinMPNN. *Science*, 378(6615), 49-56.

**DOI**: [10.1126/science.add2187](https://doi.org/10.1126/science.add2187)

**Impact on Our Model**: ‚≠ê‚≠ê
- Alternative design approach
- Informed sequence generation strategies

---

### 20. RFdiffusion (2023)

**Full Citation**:
> Watson, J. L., Juergens, D., Bennett, N. R., Trippe, B. L., Yim, J., Eisenach, H. E., ... & others (2023). De novo design of protein structure and function with RFdiffusion. *Nature*, 620(7976), 1089-1100.

**DOI**: [10.1038/s41586-023-06415-8](https://doi.org/10.1038/s41586-023-06415-8)

**Impact on Our Model**: ‚≠ê‚≠ê
- Alternative diffusion-based approach
- Comparison baseline

---

## üìä Validation Methods & Benchmarking (2024)

### 21. Benchmarking Generative Models (2024) - KEY REFERENCE

**Full Citation**:
> U√ßar, T., & Malherbe, C. (2024). Benchmarking Generative Models for Antibody Design. *bioRxiv*.

**DOI**: [10.1101/2024.10.07.617023](https://doi.org/10.1101/2024.10.07.617023)

**What We Used**:
- Validation metrics comparison
- Benchmarking standards
- Metric selection guidance

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- **Validated our metric choices**
- Showed no single "best" metric exists
- Confirmed our approach is SOTA-compliant

---

### 22. AI Methods for Antibody Design Review (2024/2025)

**Full Citation**:
> Multiple Authors (2025). Artificial intelligence-driven computational methods for antibody design and optimization. *mAbs*, 17(1).

**DOI**: [10.1080/19420862.2025.2528902](https://doi.org/10.1080/19420862.2025.2528902)

**What We Used**:
- Comprehensive validation methodology review
- In silico vs in vitro metrics
- Current challenges and solutions

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- **Primary validation reference**
- Informed our validation strategy
- Showed our approach matches current consensus

---

### 23. Deep Learning Antibody Optimization Review (2024)

**Full Citation**:
> Multiple Authors (2024). Recent advances in antibody optimization based on deep learning methods. *PMC*, 12119181.

**URL**: [PMC Article](https://pmc.ncbi.nlm.nih.gov/articles/PMC12119181/)

**What We Used**:
- Dataset and algorithm review
- Current challenges
- Best practices

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê
- Informed data preparation
- Validated model architecture choices

---

### 24. Antibody Design Review (Briefings in Bioinformatics, 2024)

**Full Citation**:
> Multiple Authors (2024). Antibody design using deep learning: from sequence and structure design to affinity maturation. *Briefings in Bioinformatics*, 25(4), bbae307.

**URL**: [Oxford Academic](https://academic.oup.com/bib/article/25/4/bbae307/7705535)

**What We Used**:
- Sequence to affinity maturation workflow
- Evaluation metrics
- Current SOTA methods

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê
- End-to-end pipeline reference
- Validated our affinity conditioning approach

---

## üíæ Datasets & Databases

### 25. Observed Antibody Space (OAS) (2018)

**Full Citation**:
> Kovaltsuk, A., Leem, J., Kelm, S., Snowden, J., Deane, C. M., & Krawczyk, K. (2018). Observed Antibody Space: A resource for data mining next-generation sequencing of antibody repertoires. *The Journal of Immunology*, 201(8), 2502-2509.

**DOI**: [10.4049/jimmunol.1800708](https://doi.org/10.4049/jimmunol.1800708)
**URL**: [OAS Database](http://opig.stats.ox.ac.uk/webapps/oas/)

**Contents**: 2+ billion antibody sequences

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê
- Identified for potential pre-training (future)
- Source of validation antibodies

---

### 26. Protein Data Bank (PDB) (2000)

**Full Citation**:
> Berman, H. M., Westbrook, J., Feng, Z., Gilliland, G., Bhat, T. N., Weissig, H., ... & Bourne, P. E. (2000). The protein data bank. *Nucleic acids research*, 28(1), 235-242.

**DOI**: [10.1093/nar/28.1.235](https://doi.org/10.1093/nar/28.1.235)
**URL**: [RCSB PDB](https://www.rcsb.org/)

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê
- Structural validation reference
- Antibody-antigen complex structures

---

### 27. SAbDab - Structural Antibody Database (2014)

**URL**: [SAbDab](https://opig.stats.ox.ac.uk/webapps/sabdab-sabpred/)

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê
- Curated antibody structures
- Validation reference dataset

---

### 28. IMGT - International ImMunoGeneTics (2001)

**URL**: [IMGT](http://www.imgt.org/)

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê
- Antibody numbering system
- CDR definition standard

---

## üíª Software & Tools

### 29. PyTorch (2019) - IMPLEMENTATION FRAMEWORK

**Full Citation**:
> Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., ... & others (2019). PyTorch: An imperative style, high-performance deep learning library. *Advances in Neural Information Processing Systems*, 32, 8024-8035.

**URL**: [PyTorch](https://pytorch.org/)

**What We Used**:
- ‚úÖ **All model implementation**
- ‚úÖ nn.Transformer
- ‚úÖ Training utilities
- ‚úÖ CUDA support

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- **Core implementation framework**
- Entire model built on PyTorch

---

### 30. Hugging Face Transformers (2020)

**Full Citation**:
> Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., ... & others (2020). Transformers: State-of-the-art natural language processing. *EMNLP: System Demonstrations*, 38-45.

**URL**: [Hugging Face](https://huggingface.co/docs/transformers)

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê
- Architecture patterns
- Implementation reference

---

### 31. fair-esm (ESMFold) (2022) - VALIDATION TOOL

**Repository**: [GitHub - ESM](https://github.com/facebookresearch/esm)

**What We Used**:
- ‚úÖ **ESMFold for structure prediction**
- ‚úÖ pLDDT confidence scores

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- **Primary validation tool**
- Fast, accurate structure prediction

---

### 32. IgFold Software (2023)

**Repository**: [GitHub - IgFold](https://github.com/Graylab/IgFold)

**Status**: ‚è≥ Identified for future use

**Impact on Our Model**: ‚≠ê‚≠ê‚≠ê (Future)

---

## üìÖ Implementation Timeline

### Phase 1: Architecture Design (Oct 31, 2024)
**Based on**:
- Vaswani et al. 2017 (Transformer)
- Xiong et al. 2020 (Pre-LN)
- Devlin et al. 2019 (GELU)
- PALM-H3 2024 (Architecture insights)

**Implemented**:
- ‚úÖ Transformer Seq2Seq
- ‚úÖ Pre-Layer Normalization
- ‚úÖ GELU activation
- ‚úÖ Affinity conditioning

---

### Phase 2: Training Optimization (Oct 31, 2024)
**Based on**:
- Loshchilov & Hutter 2017 (Cosine LR)
- Szegedy et al. 2016 (Label smoothing)
- Loshchilov & Hutter 2019 (AdamW)
- Pascanu et al. 2013 (Gradient clipping)

**Implemented**:
- ‚úÖ Warm-up + Cosine schedule
- ‚úÖ Label smoothing (0.1)
- ‚úÖ AdamW optimizer
- ‚úÖ Gradient clipping

---

### Phase 3: Validation System (Nov 3, 2024)
**Based on**:
- Lin et al. 2022 (ESMFold)
- U√ßar & Malherbe 2024 (Benchmarking)
- 2024 Reviews (Validation methods)

**Implemented**:
- ‚úÖ ESMFold integration
- ‚úÖ pLDDT-based validation
- ‚úÖ Comprehensive metrics

---

### Phase 4: Future Enhancements (Planned)
**Based on**:
- Abramson et al. 2024 (AlphaFold3 - when available)
- Ruffolo et al. 2023 (IgFold)
- Su et al. 2021 (RoPE)

**To Implement**:
- ‚è≥ AlphaFold3 binding prediction
- ‚è≥ IgFold antibody-specific validation
- ‚è≥ Rotary position embeddings

---

## üìä Model Statistics & Results

### Training Dataset
- **Source**: Custom antibody-antigen pairs
- **Size**: 158,000 pairs
  - Training: 126,508 (80%)
  - Validation: 31,602 (20%)

### Model Configuration
- **Architecture**: Transformer Seq2Seq
- **Parameters**: 5,616,153 (5.6M)
- **Embedding Size**: 256
- **Encoder/Decoder Layers**: 3 each
- **Attention Heads**: 8
- **Feedforward Dim**: 1024

### Training Progress (As of Nov 3, 2024)
- **Current Epoch**: 9/20 (45% complete)
- **Training Loss**: 0.6540
- **Validation Loss**: 0.6539
- **Sequence Validity**: 100%
- **Diversity**: 32% (growing)

### Key Results
- ‚úÖ 46% loss reduction (Epoch 1 ‚Üí 9)
- ‚úÖ 100% valid sequences
- ‚úÖ Increasing diversity (13% ‚Üí 32%)
- ‚úÖ Stable training (no overfitting)

---

## üéØ Novel Contributions

Based on literature review, this work contributes:

1. **Affinity-Conditioned Generation** üåü
   - Most models (IgLM, PALM-H3) don't control binding strength
   - Our pKd conditioning is unique

2. **Full Antibody Generation** üåü
   - PALM-H3 only generates CDR-H3
   - We generate complete heavy + light chains

3. **Modern Training Techniques** ‚úÖ
   - Combination of 2024 SOTA methods
   - Pre-LN + GELU + Warm-up/Cosine + Label Smoothing

4. **Practical Validation** ‚úÖ
   - Research-aligned without unavailable tools
   - ESMFold-based structure validation

---

## üìù Citation for This Work

If using this model or methodology, please cite:

```bibtex
@software{antibody_generation_2025,
  title={Affinity-Conditioned Antibody Generation using Transformer Seq2Seq},
  author={[Your Name]},
  year={2025},
  note={Transformer-based model for antibody generation with pKd conditioning},
  url={[GitHub/Project URL]},
  architecture={Pre-LN Transformer with GELU},
  parameters={5.6M},
  dataset={158k antibody-antigen pairs},
  validation={ESMFold + comprehensive metrics}
}
```

And cite the foundational papers listed in this document.

---

## üìñ Additional Reading

### Reviews & Surveys
1. Norman et al. 2020 - Computational antibody design overview
2. 2024 mAbs Review - AI-driven antibody methods
3. 2024 Briefings in Bioinformatics - Deep learning for antibodies

### Benchmarking Studies
4. U√ßar & Malherbe 2024 - Generative model benchmarking
5. 2024 PMC Reviews - Recent advances

### Experimental Validation
6. 2024 PMC - Deep learning with experimental validation
7. Co et al. 2022 - ML for affinity optimization

---

## üîÑ Updates & Revisions

**Version 1.0** (Nov 3, 2024)
- Initial compilation of all references
- Complete citation information
- BibTeX file created
- Implementation timeline documented

**Future Updates**:
- Add experimental validation papers (when applicable)
- Update with AlphaFold3 when API available
- Add citations for any new techniques implemented

---

**For questions about citations or to report missing references, please contact [Your Email]**

**Last Research Database Check**: November 3, 2024
**Total References**: 32 papers + 4 databases + 4 software tools = **40 sources**

---


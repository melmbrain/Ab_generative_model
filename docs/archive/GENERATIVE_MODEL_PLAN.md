# Generative Model Training Plan: Antigen â†’ Antibody

**Goal**: Train a model to generate antibody sequences when given an antigen sequence

**Dataset**: 159,735 Ab-Ag pairs from AgAb database

**Location**: `/mnt/c/Users/401-24/Desktop/Docking prediction/data/raw/agab/`

---

## ğŸ“Š Available Data

### Dataset Statistics

**File**: `agab_phase2_full.csv` (127 MB, 159,735 pairs)

**Columns**:
- `antibody_sequence`: Heavy chain | Light chain (concatenated with |)
- `antigen_sequence`: Full antigen amino acid sequence
- `pKd`: Binding affinity (-log KD)
- `affinity_type`: Measurement type
- `dataset`: Source database (95.4% from ABBD)
- `confidence`: high/medium/low
- `nanobody`: Boolean flag

**Data Quality**:
- âœ… 159,735 pairs total
- âœ… pKd range: -2.96 to 12.43
- âœ… Mean pKd: 7.45 Â± 2.11
- âœ… All sequences validated
- âœ… High-confidence subset available

---

## ğŸ¯ Modeling Approaches

### Approach 1: Sequence-to-Sequence Transformer (RECOMMENDED)

**Architecture**: Encoder-Decoder Transformer

```
Input:  [Antigen Sequence] + [Desired pKd]
           â†“
Encoder (6 layers, 512 dims)
  - Learned embeddings for amino acids
  - Positional encoding
  - Self-attention over antigen
           â†“
Latent representation (512 dims)
           â†“
Decoder (6 layers, 512 dims)
  - Cross-attention to antigen
  - Self-attention over generated antibody
  - Causal masking for autoregressive generation
           â†“
Output: [Antibody Sequence] (token-by-token)
```

**Training Objective**:
- Primary: Cross-entropy loss (sequence prediction)
- Secondary: MSE loss (pKd prediction from generated sequence)
- Auxiliary: Discriminator reward (RL fine-tuning)

**Why this approach?**
- âœ… Standard for sequence generation (proven in NLP)
- âœ… Can condition on both antigen + desired affinity
- âœ… Generates diverse candidates via beam search/sampling
- âœ… 159k samples is sufficient for training

---

### Approach 2: Conditional VAE (Variational Autoencoder)

**Architecture**:

```
Encoder:
  Antibody sequence â†’ ESM-2 â†’ 480 dims â†’ Î¼, Ïƒ (latent)
  Antigen sequence â†’ ESM-2 â†’ 480 dims (condition)

Latent Space:
  z ~ N(Î¼, Ïƒ)  [256 dims]

Decoder:
  [z + antigen_features] â†’ Transformer Decoder â†’ Antibody sequence
```

**Training Objective**:
- ELBO loss: Reconstruction + KL divergence
- Affinity matching: |predicted_pKd - target_pKd|

**Advantages**:
- âœ… Latent space allows interpolation
- âœ… Can sample diverse antibodies from z
- âœ… Regularization via KL prevents mode collapse

**Disadvantages**:
- âŒ More complex to train
- âŒ KL collapse is common

---

### Approach 3: Fine-tune Protein Language Model

**Base Model**: ESM-2 (650M params) or ProtGPT2

**Strategy**: Fine-tune on Ab-Ag pairs

```
Input format (text):
"<ANTIGEN>MKTFLIS...VYQAG</ANTIGEN><PKD>8.5</PKD><ANTIBODY>"

Target:
"QVQLVQ...TVSS|DIQMTQ...VEIK</ANTIBODY>"
```

**Training**:
- Use existing pre-trained weights
- Fine-tune on 159k pairs
- Causal language modeling objective

**Advantages**:
- âœ… Leverages pre-training (millions of proteins)
- âœ… Fast convergence
- âœ… State-of-the-art protein understanding

**Disadvantages**:
- âŒ Requires GPU (model is large)
- âŒ May not follow constraints exactly

---

## ğŸ—ï¸ Implementation Roadmap

### Phase 1: Data Preparation (Week 1)

**Tasks**:
1. âœ… Load 159k dataset from `agab_phase2_full.csv`
2. âœ… Clean sequences (validate amino acids)
3. âœ… Split heavy/light chains (currently concatenated with `|`)
4. âœ… Create train/val/test splits (80/10/10)
5. âœ… Tokenize sequences (amino acid â†’ integer mapping)
6. âœ… Compute sequence length statistics
7. âœ… Create PyTorch DataLoader

**Data Format**:
```python
{
    'antigen_seq': "MKTFLIS...",
    'antibody_heavy': "QVQLVQ...",
    'antibody_light': "DIQMTQ...",
    'pKd': 8.5,
    'affinity': 3.16e-9  # in M
}
```

**Outputs**:
- `data/generative/train.pkl` (127,788 pairs)
- `data/generative/val.pkl` (15,973 pairs)
- `data/generative/test.pkl` (15,974 pairs)

---

### Phase 2: Model Implementation (Week 2-3)

**Step 1: Implement Transformer**

```python
class AntigenToAntibodyTransformer(nn.Module):
    """
    Encoder-Decoder Transformer for Ab generation
    """
    def __init__(self):
        self.encoder = TransformerEncoder(
            d_model=512,
            nhead=8,
            num_layers=6,
            dim_feedforward=2048
        )

        self.decoder = TransformerDecoder(
            d_model=512,
            nhead=8,
            num_layers=6,
            dim_feedforward=2048
        )

        # Amino acid embeddings
        self.aa_embedding = nn.Embedding(21, 512)  # 20 AA + padding

        # Affinity conditioning
        self.affinity_proj = nn.Linear(1, 512)

    def forward(self, antigen, target_pKd, antibody=None):
        # Encode antigen
        antigen_emb = self.aa_embedding(antigen)
        antigen_emb = antigen_emb + self.affinity_proj(target_pKd)

        encoder_out = self.encoder(antigen_emb)

        # Decode antibody (teacher forcing during training)
        if antibody is not None:
            # Training: use ground truth
            antibody_emb = self.aa_embedding(antibody)
            decoder_out = self.decoder(
                antibody_emb,
                memory=encoder_out,
                tgt_mask=causal_mask
            )
        else:
            # Inference: autoregressive generation
            decoder_out = self.generate(encoder_out, max_len=150)

        return decoder_out

    def generate(self, encoder_out, max_len=150):
        """Autoregressive generation"""
        generated = [START_TOKEN]

        for i in range(max_len):
            # Decode one token at a time
            tgt_emb = self.aa_embedding(torch.tensor(generated))
            out = self.decoder(tgt_emb, encoder_out)

            # Sample next token
            next_token = torch.argmax(out[-1])

            if next_token == END_TOKEN:
                break

            generated.append(next_token)

        return generated
```

**Step 2: Training Loop**

```python
def train_generative_model():
    model = AntigenToAntibodyTransformer()
    optimizer = Adam(model.parameters(), lr=1e-4)

    for epoch in range(50):
        for batch in train_loader:
            antigen = batch['antigen_seq']
            antibody = batch['antibody_seq']
            pKd = batch['pKd']

            # Forward pass
            logits = model(antigen, pKd, antibody)

            # Compute loss
            loss = cross_entropy(logits, antibody)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        # Validation
        val_loss = evaluate(model, val_loader)
        print(f"Epoch {epoch}: Loss={val_loss:.4f}")

        # Generate sample
        sample = model.generate(val_antigen, target_pKd=9.0)
        print(f"Sample: {tokens_to_sequence(sample)}")
```

---

### Phase 3: Training (Week 3-4)

**Hardware Requirements**:
- GPU: RTX 3090 or better (24GB VRAM)
- Alternative: Google Colab Pro / AWS p3.2xlarge
- Training time: ~10-20 hours for 50 epochs

**Training Configuration**:
```python
config = {
    'd_model': 512,
    'nhead': 8,
    'num_encoder_layers': 6,
    'num_decoder_layers': 6,
    'dim_feedforward': 2048,
    'dropout': 0.1,

    'batch_size': 32,
    'learning_rate': 1e-4,
    'epochs': 50,
    'gradient_clip': 1.0,

    'max_antigen_len': 500,
    'max_antibody_len': 150,
}
```

**Monitoring**:
- Training loss (cross-entropy)
- Validation loss
- Sample generations (qualitative)
- Sequence validity (% valid AA sequences)
- Diversity (unique sequences generated)

---

### Phase 4: Validation with Discriminator (Week 4)

**Hybrid Validation**: Use existing discriminator to score generated antibodies

```python
def validate_with_discriminator():
    """
    Generate antibodies and score with discriminator
    """
    generator = AntigenToAntibodyTransformer.load('checkpoints/best.pth')
    discriminator = AffinityDiscriminator()

    results = []

    for test_antigen in test_set:
        # Generate 100 candidates
        candidates = []
        for _ in range(100):
            ab = generator.generate(
                antigen=test_antigen,
                target_pKd=9.0,  # Request high-affinity
                temperature=1.0   # Diversity
            )
            candidates.append(ab)

        # Score with discriminator
        scores = []
        for ab in candidates:
            score = discriminator.predict_single(ab, test_antigen)
            scores.append(score['predicted_pKd'])

        results.append({
            'antigen': test_antigen,
            'mean_pKd': np.mean(scores),
            'max_pKd': np.max(scores),
            'diversity': len(set(candidates)) / len(candidates)
        })

    return results
```

**Metrics**:
1. **Affinity accuracy**: How close is generated pKd to requested?
2. **Discriminator score**: Do generated Abs score well?
3. **Sequence validity**: % of valid sequences
4. **Diversity**: Unique sequences generated
5. **CDR quality**: Are CDR regions reasonable?

---

### Phase 5: Production Deployment (Week 5)

**API Design**:

```python
class AntigenToAntibodyAPI:
    """
    Production API for antibody generation
    """

    def __init__(self):
        self.generator = AntigenToAntibodyTransformer.load()
        self.discriminator = AffinityDiscriminator()

    def design_antibodies(
        self,
        antigen_seq: str,
        target_pKd: float = 9.0,
        n_candidates: int = 100,
        diversity: float = 1.0
    ):
        """
        Generate antibodies for antigen

        Args:
            antigen_seq: Target antigen sequence
            target_pKd: Desired binding affinity
            n_candidates: Number of antibodies to generate
            diversity: Sampling temperature (0.1-2.0)

        Returns:
            List of antibody candidates with scores
        """
        # Generate candidates
        candidates = []
        for _ in range(n_candidates):
            ab = self.generator.generate(
                antigen=antigen_seq,
                target_pKd=target_pKd,
                temperature=diversity
            )
            candidates.append(ab)

        # Re-rank with discriminator
        scored = []
        for ab in candidates:
            score = self.discriminator.predict_single(ab, antigen_seq)
            scored.append({
                'antibody_sequence': ab,
                'predicted_pKd': score['predicted_pKd'],
                'predicted_Kd_nM': score['predicted_Kd_nM'],
                'category': score['binding_category']
            })

        # Sort by affinity
        scored = sorted(scored, key=lambda x: x['predicted_pKd'], reverse=True)

        return scored
```

---

## ğŸ“Š Expected Performance

### Generation Quality

**Best case** (based on similar work):
- Sequence validity: 95%+ (valid amino acid sequences)
- Affinity correlation: Ï = 0.6-0.7 (generated pKd vs requested)
- Discriminator approval: 60%+ score > 7.0 pKd
- Diversity: 80%+ unique sequences

**Realistic case**:
- Sequence validity: 90%+
- Affinity correlation: Ï = 0.4-0.6
- Discriminator approval: 40-60% good binders
- Diversity: 70%+ unique

**Minimum acceptable**:
- Sequence validity: 85%+
- Affinity correlation: Ï > 0.3
- Discriminator approval: 30%+ good binders
- Diversity: 50%+ unique

### Comparison to Baselines

| Method | Affinity Control | Diversity | Speed |
|--------|-----------------|-----------|-------|
| Template mutation | âŒ None | Low | Fast |
| Guided search | âœ… Good | Medium | Slow |
| **Seq2Seq** | âœ…âœ… Excellent | High | Fast |
| DiffAb | âœ… Good | High | Medium |

---

## ğŸ’» Code Structure

```
Ab_generative_model/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ generative/
â”‚       â”œâ”€â”€ train.pkl              # 127k training pairs
â”‚       â”œâ”€â”€ val.pkl                # 16k validation pairs
â”‚       â””â”€â”€ test.pkl               # 16k test pairs
â”‚
â”œâ”€â”€ generators/
â”‚   â”œâ”€â”€ template_generator.py      # Existing
â”‚   â”œâ”€â”€ guided_search.py           # Phase 1 (quick win)
â”‚   â””â”€â”€ seq2seq_generator.py       # NEW - Transformer model
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_generative_data.py # Data preprocessing
â”‚   â”œâ”€â”€ train_seq2seq.py           # Training script
â”‚   â”œâ”€â”€ evaluate_generator.py      # Validation
â”‚   â””â”€â”€ generate_antibodies.py     # Inference API
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ generative/
â”‚       â”œâ”€â”€ config.json            # Model hyperparameters
â”‚       â”œâ”€â”€ best_model.pth         # Trained weights
â”‚       â””â”€â”€ training_log.csv       # Training history
â”‚
â””â”€â”€ notebooks/
    â”œâ”€â”€ data_exploration.ipynb     # EDA on 159k dataset
    â”œâ”€â”€ model_training.ipynb       # Interactive training
    â””â”€â”€ generation_demo.ipynb      # Demo notebook
```

---

## ğŸš€ Quick Start: Data Preparation

**Step 1: Prepare dataset**

```python
import pandas as pd
import pickle
from sklearn.model_selection import train_test_split

# Load 159k pairs
df = pd.read_csv(
    '/mnt/c/Users/401-24/Desktop/Docking prediction/data/raw/agab/agab_phase2_full.csv'
)

# Split antibody sequence (heavy|light)
df[['heavy', 'light']] = df['antibody_sequence'].str.split('|', expand=True)

# Clean
df = df.dropna(subset=['heavy', 'antigen_sequence', 'pKd'])
df = df[df['pKd'] > 0]  # Remove invalid affinities

# Split
train, temp = train_test_split(df, test_size=0.2, random_state=42)
val, test = train_test_split(temp, test_size=0.5, random_state=42)

# Save
train.to_pickle('data/generative/train.pkl')
val.to_pickle('data/generative/val.pkl')
test.to_pickle('data/generative/test.pkl')

print(f"Train: {len(train)}")
print(f"Val: {len(val)}")
print(f"Test: {len(test)}")
```

**Step 2: Tokenization**

```python
# Amino acid vocabulary
AA_VOCAB = {
    '<PAD>': 0, '<START>': 1, '<END>': 2,
    'A': 3, 'C': 4, 'D': 5, 'E': 6, 'F': 7,
    'G': 8, 'H': 9, 'I': 10, 'K': 11, 'L': 12,
    'M': 13, 'N': 14, 'P': 15, 'Q': 16, 'R': 17,
    'S': 18, 'T': 19, 'V': 20, 'W': 21, 'Y': 22
}

def tokenize_sequence(seq):
    return [AA_VOCAB['<START>']] + \
           [AA_VOCAB[aa] for aa in seq] + \
           [AA_VOCAB['<END>']]

def detokenize_sequence(tokens):
    REVERSE_VOCAB = {v: k for k, v in AA_VOCAB.items()}
    seq = [REVERSE_VOCAB[t] for t in tokens if t > 2]
    return ''.join(seq)
```

---

## ğŸ“ˆ Timeline & Milestones

### Week 1: Data Preparation
- [x] Explore 159k dataset structure
- [ ] Implement data preprocessing pipeline
- [ ] Create train/val/test splits
- [ ] Compute sequence statistics
- [ ] Build PyTorch DataLoader

### Week 2: Model Implementation
- [ ] Implement Transformer encoder
- [ ] Implement Transformer decoder
- [ ] Add affinity conditioning
- [ ] Test forward/backward pass
- [ ] Implement generation (beam search)

### Week 3: Initial Training
- [ ] Train on 10k subset (quick test)
- [ ] Debug any issues
- [ ] Train on full 127k dataset
- [ ] Monitor convergence
- [ ] Tune hyperparameters

### Week 4: Validation
- [ ] Generate test antibodies
- [ ] Score with discriminator
- [ ] Analyze sequence quality
- [ ] Compare to baselines
- [ ] Iterate if needed

### Week 5: Production
- [ ] Clean up code
- [ ] Create API
- [ ] Write documentation
- [ ] Deploy model
- [ ] Create demo notebook

---

## ğŸ¯ Success Criteria

**Minimum Viable Model**:
- âœ… Generates valid sequences (90%+)
- âœ… Better than random baseline
- âœ… Some antibodies score well (30%+)

**Good Model**:
- âœ… Affinity correlation Ï > 0.5
- âœ… 50%+ antibodies score pKd > 7.0
- âœ… High diversity (70%+ unique)

**Excellent Model**:
- âœ… Affinity correlation Ï > 0.7
- âœ… 70%+ antibodies score pKd > 7.0
- âœ… Beats all baselines
- âœ… Publication-worthy

---

## ğŸ“š References

**Similar Work**:
1. **DiffAb** (2022): Diffusion models for CDR design
2. **IgLM** (2022): Language models for antibody generation
3. **AbDPO** (2023): Preference optimization for antibodies
4. **dyMEAN** (2023): Deep learning for Ab-Ag binding

**Our Advantage**:
- âœ… 159k training pairs (more than most)
- âœ… Validated discriminator for re-ranking
- âœ… End-to-end pipeline

---

## ğŸ”§ Next Steps

**Immediate** (Today):
1. Run data preparation script
2. Explore sequence statistics
3. Design model architecture

**This Week**:
1. Implement Transformer
2. Test on small subset (1k pairs)
3. Debug and iterate

**Next Week**:
1. Train on full dataset
2. Validate with discriminator
3. Compare to baselines

---

**Want me to start implementing the data preparation pipeline?**

I can create:
1. `scripts/prepare_generative_data.py` - Data preprocessing
2. `notebooks/data_exploration.ipynb` - EDA notebook
3. `generators/seq2seq_generator.py` - Model skeleton

Let me know if you want to proceed! ğŸš€
